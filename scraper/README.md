# Scraper Module

1. æ•´åˆäº† **Tavily API** é€²è¡ŒéåŒæ­¥ç¶²è·¯æœå°‹ã€‚
2. å…·å‚™è‡ªå‹•åŒ–çš„æœ¬åœ°å¿«å–ï¼ˆLocal Cacheï¼‰èˆ‡æª”æ¡ˆç´¢å¼•ç®¡ç†åŠŸèƒ½ï¼Œèƒ½æœ‰æ•ˆç¯€çœ API é¡åº¦ä¸¦åŠ é€Ÿé‡è¤‡æŸ¥è©¢çš„å›æ‡‰æ™‚é–“ã€‚

##  è³‡æ–™å„²å­˜ä½ç½® (Evidence Location)

æ‰€æœ‰çš„æœå°‹çµæœè­‰æ“šï¼ˆEvidenceï¼‰èˆ‡ç´¢å¼•æª”æ¡ˆæœƒè‡ªå‹•å„²å­˜æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„ `data/evidence` è³‡æ–™å¤¾ä¸­ã€‚

* **å„²å­˜è·¯å¾‘**: `project_root/data/evidence/`
* **æª”æ¡ˆçµæ§‹**:
    * `index.json`: è¨˜éŒ„ Query String èˆ‡å°æ‡‰æª”æ¡ˆåç¨±çš„æ˜ å°„è¡¨ (Key-Value Map)ã€‚
    * `evidence.json`, `evidence1.json`...: å¯¦éš›å­˜æ”¾æœå°‹çµæœçš„ JSON æª”æ¡ˆã€‚

> **æ³¨æ„**: æ¨¡çµ„æœƒè‡ªå‹•å»ºç«‹æ­¤è³‡æ–™å¤¾çµæ§‹ï¼Œç„¡éœ€æ‰‹å‹•å»ºç«‹ã€‚

---

## å®‰è£èˆ‡è¨­å®š

1.  **ä¾è³´å¥—ä»¶**:
    è«‹ç¢ºä¿å·²å®‰è£ `tavily-python`ã€‚
    ```bash
    pip install tavily-python
    ```

2.  **API Key è¨­å®š**:
    æœ¬æ¨¡çµ„ä¾è³´ Tavily APIã€‚è«‹åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼ˆèˆ‡ `scraper/` åŒå±¤ï¼‰å»ºç«‹ä¸€å€‹åç‚º `API_KEY.py` çš„æª”æ¡ˆï¼Œå…§å®¹å¦‚ä¸‹ï¼š
    ```python
    # API_KEY.py
    TAVILY_API_KEY = "ä½ çš„_Tavily_API_Key"
    ```

---

## æ¨¡çµ„ä½¿ç”¨èªªæ˜

ä½ å¯ä»¥ç›´æ¥å¾ `scraper` å¥—ä»¶å¼•å…¥ä¸»è¦çš„ Handlerï¼š
```python
from scraper import EvidenceRetrieveHandler, EvidenceFileHandler
```

### 1. EvidenceRetrieveHandler (æ ¸å¿ƒæœå°‹æ§åˆ¶å™¨)
è² è²¬ç®¡ç†éåŒæ­¥æœå°‹ä»»å‹™ã€åŸ·è¡Œç·’æ±  (Thread Pool) ä»¥åŠå¿«å–ç­–ç•¥ã€‚

#### åˆå§‹åŒ–
```Python
handler = EvidenceRetrieveHandler(max_search_requests=5)
```
`max_search_requests (int)`: æœ€å¤§ä½µç™¼è«‹æ±‚æ•¸ (`Default: 5`)ã€‚

#### Method : `query`
åŸ·è¡Œæœå°‹è«‹æ±‚ã€‚æœƒæ ¹æ“šè¨­å®šæ±ºå®šè®€å–å¿«å–æˆ–ç™¼èµ·æ–°çš„ API è«‹æ±‚ã€‚
```Python
result = handler.query(
    query: dict, 
    use_local_TF: bool = False, 
    chunk_count: int = 3, 
    result_count: int = 3, 
    level: str = "basic"
)
```
åƒæ•¸èªªæ˜:
    `query (dict)`: å¿…é ˆåŒ…å« `"query"` (æœå°‹é—œéµå­—)ã€‚å¯é¸æ¬„ä½åŒ…å« `"search_region"`, `"search_duration"`ã€‚
    `use_local_TF (bool)`: æ˜¯å¦å„ªå…ˆæª¢æŸ¥æœ¬åœ°å¿«å–ã€‚è‹¥ç‚º `True` ä¸”å¿«å–å­˜åœ¨ï¼Œç›´æ¥å›å‚³æª”æ¡ˆ `Handlerã€‚`
    `chunk_count (int)`: æ¯å€‹ä¾†æºæ“·å–çš„ç‰‡æ®µæ•¸ (`Default: 3`)ã€‚
    `result_count (int)`: API å›å‚³çš„æœ€å¤§çµæœæ•¸ (`Default: 3`)ã€‚
    `level (str)`: æœå°‹æ·±åº¦ï¼Œå¯é¸ `"basic"` æˆ– `"advanced"`ã€‚

å›å‚³å€¼ (Return Types):
    `Future`: è‹¥ç™¼èµ·æ–°çš„ API è«‹æ±‚ (éåŒæ­¥ç‰©ä»¶)ã€‚
    `EvidenceFileHandler`: è‹¥å‘½ä¸­æœ¬åœ°å¿«å– (åŒæ­¥ç‰©ä»¶)ã€‚
    `None`: è‹¥æŸ¥è©¢ç„¡æ•ˆã€‚

#### Method : `shutdown`
é—œé–‰åŸ·è¡Œç·’æ± ã€‚
```Python
handler.shutdown(wait=True, cancel_futures=True)
```


### 2. EvidenceFileHandler (æª”æ¡ˆå­˜å–ç®¡ç†å™¨)
è² è²¬è­‰æ“šæª”æ¡ˆçš„è®€å–ã€å¯«å…¥èˆ‡ç´¢å¼•æŸ¥æ‰¾ã€‚

#### Static Method : `store`
å°‡è³‡æ–™å¯«å…¥æª”æ¡ˆä¸¦æ›´æ–° `index.json`ã€‚
```Python
handler = EvidenceFileHandler.store(data: dict)
```
`data`: å¿…é ˆåŒ…å« `"query"` æ¬„ä½ä»¥å»ºç«‹ç´¢å¼•ã€‚

#### Static Method : `find_query`
é€é Query String å°‹æ‰¾æ—¢æœ‰çš„è­‰æ“šæª”æ¡ˆã€‚
```Python
handler = EvidenceFileHandler.find_query(query: str)
```
`å›å‚³`: è‹¥æ‰¾åˆ°å›å‚³ `EvidenceFileHandler` ï¼Œå¦å‰‡å›å‚³ `Noneã€‚`

#### Method : `read`
è®€å–æª”æ¡ˆå…§å®¹ã€‚
```Python
content = handler.read()  # å›å‚³ dict
```

#### Method : `close`
é—œé–‰æª”æ¡ˆä¸²æµã€‚é‡è¦ï¼šä½¿ç”¨å®Œç•¢å¾Œè«‹å‹™å¿…å‘¼å«ã€‚
```Python
handler.close()
```

#### Method : `get_filename`
å–å¾—ç›®å‰é–‹å•Ÿçš„æª”æ¡ˆåç¨±ã€‚
```Python
filename = handler.get_filename()
```

---

## ä½¿ç”¨ç¯„ä¾‹ (Examples)

### ç¯„ä¾‹ 1: éåŒæ­¥æœå°‹ (Async Search)
æ­¤ç¯„ä¾‹å±•ç¤ºå¦‚ä½•æ‰¹é‡è™•ç†æŸ¥è©¢ï¼Œå€åˆ†ã€Œå¿«å–çµæœã€èˆ‡ã€ŒAPI è«‹æ±‚çµæœã€ã€‚

```Python
from scraper import EvidenceRetrieveHandler, EvidenceFileHandler
from concurrent.futures import Future

# å®šç¾©æŸ¥è©¢æ¸…å–®
queries = [
    {
        "search_region": "Taiwan",
        "search_duration": "all_time",
        "query": "å°ç£ æ±éƒ¨ æµ·åŸŸ 7.0åœ°éœ‡"
    },
    {
        "search_region": "US",
        "search_duration": "all_time",
        "query": "é¦¬æ–¯å…‹ æœ€æ–°æ–°è"
    }
]

# åˆå§‹åŒ– Handler
handler = EvidenceRetrieveHandler(max_search_requests=2)
wait_list = []

print("ğŸš€ é–‹å§‹æœå°‹ä»»å‹™...")

for q in queries:
    print(f"æ­£åœ¨è™•ç†: {q['query']}")
    
    # ç™¼é€æŸ¥è©¢ (å•Ÿç”¨æœ¬åœ°å¿«å–)
    result = handler.query(
        query=q, 
        use_local_TF=True, 
        level=EvidenceRetrieveHandler.ADVANCED
    )

    # æƒ…æ³ A: å‘½ä¸­å¿«å– (ç›´æ¥è®€å–)
    if isinstance(result, EvidenceFileHandler):
        data = result.read()
        print(f"âœ… [å¿«å–å‘½ä¸­] {data.get('summary', 'ç„¡æ‘˜è¦')}")
        result.close() # è¨˜å¾—é—œé–‰
        continue

    # æƒ…æ³ B: API è«‹æ±‚ä¸­ (åŠ å…¥ç­‰å¾…æ¸…å–®)
    if isinstance(result, Future):
        print("â³ [API è«‹æ±‚ä¸­] å·²åŠ å…¥æ’ç¨‹...")
        wait_list.append(result)

# ç­‰å¾… API çµæœ
print(f"\nç­‰å¾… {len(wait_list)} å€‹ç¶²è·¯è«‹æ±‚å®Œæˆ...")
for future in wait_list:
    try:
        data = future.result() # é€™è£¡æœƒé˜»å¡ç›´åˆ°å®Œæˆ
        if data:
            print(f"âœ… [API å®Œæˆ] {data.get('summary', 'ç„¡æ‘˜è¦')}")
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

handler.shutdown()
```

### ç¯„ä¾‹ 2: æ‰‹å‹•æª”æ¡ˆæ“ä½œ (Manual File Operations)
æ­¤ç¯„ä¾‹å±•ç¤ºå¦‚ä½•æ‰‹å‹•å­˜å–è­‰æ“šæª”æ¡ˆèˆ‡æ¸¬è©¦ç´¢å¼•æ©Ÿåˆ¶ã€‚

```Python
from scraper import EvidenceFileHandler

# 1. æ‰‹å‹•å­˜æª”
data = {
    "query": "æ¸¬è©¦æ‰‹å‹•å­˜æª”",
    "summary": "é€™æ˜¯ä¸€ç­†æ‰‹å‹•å¯«å…¥çš„æ¸¬è©¦è³‡æ–™ã€‚",
    "results": []
}

print("æ­£åœ¨å¯«å…¥æª”æ¡ˆ...")
handler = EvidenceFileHandler.store(data)
filename = handler.get_filename()
print(f"âœ… æª”æ¡ˆå·²å»ºç«‹: {filename}")
handler.close()

# 2. é€é Query æ‰¾å›æª”æ¡ˆ
target_query = "æ¸¬è©¦æ‰‹å‹•å­˜æª”"
print(f"\næ­£åœ¨æœå°‹ Query: {target_query}")

found_handler = EvidenceFileHandler.find_query(target_query)

if found_handler:
    content = found_handler.read()
    print(f"âœ… æˆåŠŸæ‰¾å›æª”æ¡ˆï¼å…§å®¹æ‘˜è¦: {content.get('summary')}")
    found_handler.close()
else:
    print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ")
```
